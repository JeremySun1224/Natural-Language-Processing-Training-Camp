{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们可以使用Word Embedding来解决Edit-Distance Problem的弊端。Word Embedding中文的意思是词嵌入。Embedding可以理解为它把单词放在合适的地方。这个方法不能解决一词多义的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Representation.png\", width=600, height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们怎么去表征单词? 如：ASCII, Uniicode, UTF-8。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Why.png\", width=600, height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- UTF-8是以词频来编码的。道理很简单，词频越高的单词编码越短，这样我们就能以最短的编码来表征最多的文字。\n",
    "- 为什么我们不能使用数字去表征单词？因为很简单的道理，数字是连续的，而单词是离散的。\n",
    "- 那我们能直接把单词表示成One-Hot编码吗？\n",
    "    - 对于One-Hot编码，任意两个单词之间距离都一样，所以它不会出现两个单词之间距离近了但单词意思不接近。尽管它避免出现了这样的情况，但它又损失了很多东西。例如我们判断出$f(x)$是正项的，但由于任意两个单词之间的距离相等，所以对每一个新的$x$，我们都需要重新拟合$f(x)$，所以用One-Hot编码表征单词也不行。\n",
    "    - 此外，我们汉语单词大概50万个，但是呢有3万个单词出现的频率加起来已经超过了90%，而这3万个单词里边有1万多个单词出现的频率最高。所以我们现在假设One-Hot编码维度就是$10000*10000$，于是对于这个问题我们现在要存下这些数据需要0.5个G的内存。\n",
    "<img src=\"One-Hot.png\", width=600, height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are words merely categorical?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Question.png\", width=550, height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 单词是不是一种categorical的形式呢？不是。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Answer.png\", width=600, height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 比如上图把单词表示成0101的形式，就是把单词当作是categorical形式的。由于这些向量是正交的，任意两个单词加起来再乘以另一个单词，这和直接相乘是一样的，**这样的运算不能显示出相似关系**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The problem of representing words as One-Hot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Question1.png\", width=600, height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So, we need some more dense vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Some more dense vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we do PCA of this One-Hot matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 应用One-Hot编码还是之前的问题：如果增加维度，函数就得重新写，而且随着维度的增加，所需的内存空间也会越来越大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PCA是一种降维方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"PCA1.png\", width=400, height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 从图中可以看出，有一群由$(x_1, x_2)$坐标组成的点，它们可以由$y=kx+b$近似拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"PCA2.png\", width=400, height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 由于$(x_1, x_2)$可以由$y=kx+b$近似拟合，我们可以直接将$x_1$旋转$\\theta$得到。于是$(x_1, x_2)$两个维度就变成一个维度了，实现了降维。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we get PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 可以从这个链接查看[PCA Wikiwand](https://www.wikiwand.com/en/Principal_component_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we do SVD of this One-Hot matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we get SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 可以从这个链接查看[SVD Wikiwand](https://www.wikiwand.com/en/Singular_value_decomposition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, what features do our vectors need?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Need.png\", width=600, height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如果有两个单词$w_1, w_2$，它们身边经常出现同样的或类似的单词，那么$w_1, w_2$的意思就应该挺接近的。换句话说就是，这个单词什么意思，它都可以通过它周围的单词来表征。基于这种假设，这个时候我们就提出了Embedding。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Vector.png\", width=600, height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Embedding.png\", width=600, height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 某些单词，我们把它从一个空间投影到另一个空间中。在投影到另外一个空间中时，还保存了他们之间的相似关系。Embedding就是在单词的投影过程中，我们还把它的相似性也保持住了。**就是说，如果之前$w_1, w_2$是更相似的，那么在新的空间里面，这两个向量的距离也很接近**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"V1.png\", width=600, height=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如果$\\delta{(v_1',v_2')}$ < $\\delta{(v_1',v_3')}$，那么$\\delta{(v_1,v_2)}$ < $\\delta{(v_1,v_3)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Easy.png\", width=600, height=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 上图表示：假如单词“好看”和“美丽”的词向量如后所示，$v_1和v_2$，把它们分别乘以矩阵$M$后记为$v_1'$和$v_2'$。如果有一种方法能够让$v_1'$和$v_2'$是接近的，就是让由“好看”和“美丽”生成的$v_1'$和$v_2'$是接近的，那么我们就能够把它们的词向量确定下来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 引子：对于任意一组数据的话，我们能把它表示成概率，可以用于分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Softmax.png\", width=200, heigt=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 但是这种计算方法有个弊端，对于一组差别很小的数[0.00013, 0.00022, 0.00031]，按照加和取对应倒数的方法求出的概率差别也很小，这不利于分组。后来人们提出了**Softmax**的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Softmax1.png\", width=300, heigt=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们对第一列的原始数据首先进行指数运算，一方面可以把原始数据中的附属全部化为正数，二来进行指数运算后，哪怕原始数据差别较小，也能得到差异明显的数据。\n",
    "- 由于经过变换后的数据加和为1，故我们将其抽象为**概率**。也就是说，**Softmax是将实数这样的向量变成一种概率分布**。编程实现如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector = np.array([-1, -3, 10, 14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(vec):\n",
    "    exp = np.exp(vec)\n",
    "    return exp / np.sum(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.00400195e-07, 4.06547454e-08, 1.79862038e-02, 9.82013455e-01])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如果我们把向量vector换成很大的数字时，softmax()会遇到一个问题，相除或数字太大就溢出了，显示**nan**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector = [11231, 999, 123142]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JeremySun\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:3: RuntimeWarning: invalid value encountered in true_divide\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 那么这个时候我们怎么办呢？减去向量里的最大值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_2(vec):\n",
    "    vec -= np.max(vec)\n",
    "    exp = np.exp(vec)\n",
    "    return exp / np.sum(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_2(vector)  # 显示正常"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Word2vec**是只有一个隐层的全连接神经网络，它可以用来预测给定单词的关联度大的单词。具体如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"Step1.png\", width=700, heigth=700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 对于单词“美丽”和“好看”，给它们分别随机生成两组维度为1\\*100的词向量$v_1$和$v_2$，现在我们将它们与一个100\\*10000维的随机矩阵相乘，经过Softmax处理后得到一组1\\*10000维的向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Reverse.png\", width=600, heigth=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们期望当输入“美丽”即$v_1$的时候，输出的$P_{r_1}, P_{r_2}, P_{r_3}, ..., P_{r_{10000}}$哪些数字最大呢？答：“美丽”周围的单词出现的概率最大。比如“美丽”周围的单词是$A, B, C$，那么输出结果经过排序后，我们希望前三位的是它们几个出现的概率，而且最好是1。但是这些$v_1, v_2, M$都是随机的。**那么怎么让这三个结果是1呢？**这就是一个机器学习过程：我们给定了一个输入，输入是“美丽”和$A, B, C$，输出是概率里$P_{r_1}, P_{r_2}, P_{r_3}$这几个值最大，最好是1，别的都是0。那么这个模型就会更新权值，这个上节课的$y=kx+b$更新$k, b$相似。到了“好看”的时候，“美丽”和“好看”之所以相似是因为它们周围的单词类似。那么“好看”这里的单词也会使$P_{r_1}, P_{r_2}, P_{r_3}$最大。也就是说，如果这两个单词意思相似，那么我们期望的输出也相似。因为输出的是“美丽”和“好看”周围的单词概率。哪怕它们不相似，我们也可以通过机器学习的方法让$v_1, M'$和$v_2, M'$进行多次更新，这相似（反向传播）。在这个机器过程里，因为“美丽”和“好看”用的$M$是同一个$M$，而且要求最后输出的结果也类似，所以$v_1, v_2$会逐步变成两个接近的向量。**所以，我们就通过这种过程获得了$v_1和v_2$甚至于$v_{10000}$单词的向量，整个这个算法呢就叫做Word2vec**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Word2vec**最早提出来的时候有两种方法。第一种是**Skip-Gram**，第二种是**CBOW**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 从一个单词来预测上下文"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Skip-Gram.png\", width=600, heigth=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 对于**Skip-Gram**，首先gram指的是一个一个的短单词。**Skip-Gram**在预测的时候，它把“美丽”这个单词跳过，用这个单词来预测周围的单词。也就是说，**Skip-Gram**输入的是$v_1$这个单词，输出的是它的周围会有什么单词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 从上下文来预测一个单词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"CBOW.png\", width=600, height=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 对于**CBOW**，即**Continuous Back of Words**。它的输入已经不是$v_1, v_2,...$了，而是它周围的单词，比方说$A, B, C, D$，输出的它期望的那个单词，比如“美丽”在第50个，那么它就期望在第50个输出为1。**这种方法的主要思想是：如果某单词周围的单词都类似，那么它的输出也会类似。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Word2vec的过程是这样的：现在有一个函数$f(\\vec v)$，它的输入是一个向量，这个函数将会产生一个概率分布。（之前$f(x)=kx+b$中的x是不变的，而现在呢$f(\\vec v)$中的$\\vec v$）是可变的，因为我们不知道这个$\\vec v$是什么，我们其实是想获得这个$\\vec v$，所以需要不断的迭代，故它是变化的。\n",
    "- 然后，这个$f(\\vec v)$是如何变成概率分布的呢？首先$\\vec v$和$M$相乘了，假设$\\vec v$是1\\*100维，$M$是100\\*10000维，${\\vec v} * M$将是一个1\\*10000维的向量，然后我们对这个向量进行softmax，把它变成了一个概率分布。换句话说，输入$f(\\vec v_{好看})$的结果应该约定于$f(\\vec v_{美丽})$，即$f(\\vec v_{好看}) \\approx f(\\vec v_{美丽})$。这是为什么呢？因为生成的概率分布指的是周围单词出现的概率，表示它的周围会出现什么单词。（我在输入“好看”的时候，我就已经知道“好看”的周围是什么单词。相比于之前的$f(x)=kx+b$，矩阵$M$就相当于$k$，$x$就相当于向量$\\vec v$，唯一不同的就是$\\vec v$是可变的。我们不仅要求出$\\frac {\\partial loss} {\\partial M}$，也要求出$\\frac {\\partial loss} {\\partial k}$）。\n",
    "- 再次总结：假设$f(\\vec v_{好看})$拟合出概率分布$\\vec P_1$，$f(\\vec v_{美丽})$拟合出概率分布$\\vec P_2$。$\\vec P_1$和$\\vec P_2$按照我们对周围词的理解中，它们应该是接近的。这个$\\vec P_1$和$\\vec P_2$是怎么来的呢？$$Softmax(\\vec v_1 * M) => \\vec P_1$$ $$Softmax(\\vec v_2 * M) => \\vec P_2$$ 这两个本来就应该是接近的，然后我们选择一个$Loss$不断的进行**反向传播**，把$\\vec v_1, M;\\vec v_2, M$都进行更新，因为$M$是同一个，所以到最后$\\vec v_1,\\vec v_2$就很接近了，收敛后$\\vec v_1,\\vec v_2$就不会变了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **为什么这个概率分布不能用作词向量呢？**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 答：如果你现在有10篇文章，你后又增加了3篇文章，共计13篇，那么对应的，所有单词的频率就全都变了，你又需要从第一个单词从头到尾全都算一遍。现在的话你只需要在$\\vec v_1, \\vec v_2$的基础上接着添加其他的向量即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How we use word embedding in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"How.png\", width=500, height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**基于维基百科的词向量构建**\n",
    "- 在本章，你将使用Gensim和维基百科获得你的第一批词向量，并且感受词向量的基本过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Wikipedia Chinese Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 下载[维基百科中文语料](https://dumps.wikimedia.org/zhwiki/20190720/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using wikiextractor to extrac the wikipedia corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用[Python Wikipedia Extractor](https://github.com/attardi/wikiextractor)抽取维基百科的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Gensim to get word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 参考Gensim的文档和Kaggle的参考文档，获得词向量。注意，你要使用Jieba分词把维基百科的内容切成一个一个单词，然后存进新的文件中。最后，你需要使用Gensim的LineSentence这个类进行文件的读取，再之后开始训练词向量Model。\n",
    "- 参考[Word2vec](https://radimrehurek.com/gensim/models/word2vec.html)文档"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using some words to test your preformance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 测试同义词，找几个单词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using visualization tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [visualizing-word-vectors](https://www.kaggle.com/jeffd23/visualizing-word-vectors-with-t-sne)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
